{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dict = {\n",
    "    'yelp': './Dataset/yelp_labelled.txt',\n",
    "    'imdb': './Dataset/imdb_labelled.txt',\n",
    "    'amzn': './Dataset/amazon_cells_labelled.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)\n",
    "del df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2748 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      "sentence    2748 non-null object\n",
      "label       2748 non-null int64\n",
      "source      2748 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 85.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Service was very prompt.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Would not go back.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The cashier had no care what so ever on what I...</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I tried the Cape Cod ravoli, chicken,with cran...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I was disgusted because I was pretty sure that...</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I was shocked because no signs indicate cash o...</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Highly recommended.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Waitress was a little slow in service.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This place is not worth your time, let alone V...</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>did not like at all.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Burrittos Blah!</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The food, amazing.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Service is also cute.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>I could care less... The interior is just beau...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>So they performed.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>That's right....the red velvet cake.....ohhh t...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>- They never brought a salad we asked for.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>This hole in the wall has great Mexican street...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Took an hour to get our food only 4 tables in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The worst was the salmon sashimi.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>I plugged it in only to find out not a darn th...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Excellent product.</td>\n",
       "      <td>1</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Earbud piece breaks easily.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>Lousy product.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>This phone tries very hard to do everything bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>It is the best charger I have seen on the mark...</td>\n",
       "      <td>1</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>SWEETEST PHONE!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>:-)Oh, the charger seems to work fine.</td>\n",
       "      <td>1</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>It fits so securely that the ear hook does not...</td>\n",
       "      <td>1</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Not enough volume.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Echo Problem....Very unsatisfactory</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>you could only take 2 videos at a time and the...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>don't waste your money.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>I am going to have to be the first to negative...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>Adapter does not provide enough charging current.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>There was so much hype over this phone that I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>You also cannot take pictures with it in the c...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Phone falls out easily.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>It didn't work, people can not hear me when I ...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>The text messaging feature is really tricky to...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>I'm really disappointed all I have now is a ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Painful on the ear.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Lasted one day and then blew up.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>disappointed.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Kind of flops around.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label source\n",
       "0                             Wow... Loved this place.      1   yelp\n",
       "1                                   Crust is not good.      0   yelp\n",
       "2            Not tasty and the texture was just nasty.      0   yelp\n",
       "3    Stopped by during the late May bank holiday of...      1   yelp\n",
       "4    The selection on the menu was great and so wer...      1   yelp\n",
       "5       Now I am getting angry and I want my damn pho.      0   yelp\n",
       "6                Honeslty it didn't taste THAT fresh.)      0   yelp\n",
       "7    The potatoes were like rubber and you could te...      0   yelp\n",
       "8                            The fries were great too.      1   yelp\n",
       "9                                       A great touch.      1   yelp\n",
       "10                            Service was very prompt.      1   yelp\n",
       "11                                  Would not go back.      0   yelp\n",
       "12   The cashier had no care what so ever on what I...      0   yelp\n",
       "13   I tried the Cape Cod ravoli, chicken,with cran...      1   yelp\n",
       "14   I was disgusted because I was pretty sure that...      0   yelp\n",
       "15   I was shocked because no signs indicate cash o...      0   yelp\n",
       "16                                 Highly recommended.      1   yelp\n",
       "17              Waitress was a little slow in service.      0   yelp\n",
       "18   This place is not worth your time, let alone V...      0   yelp\n",
       "19                                did not like at all.      0   yelp\n",
       "20                                 The Burrittos Blah!      0   yelp\n",
       "21                                  The food, amazing.      1   yelp\n",
       "22                               Service is also cute.      1   yelp\n",
       "23   I could care less... The interior is just beau...      1   yelp\n",
       "24                                  So they performed.      1   yelp\n",
       "25   That's right....the red velvet cake.....ohhh t...      1   yelp\n",
       "26          - They never brought a salad we asked for.      0   yelp\n",
       "27   This hole in the wall has great Mexican street...      1   yelp\n",
       "28   Took an hour to get our food only 4 tables in ...      0   yelp\n",
       "29                   The worst was the salmon sashimi.      0   yelp\n",
       "..                                                 ...    ...    ...\n",
       "970  I plugged it in only to find out not a darn th...      0   amzn\n",
       "971                                 Excellent product.      1   amzn\n",
       "972                        Earbud piece breaks easily.      0   amzn\n",
       "973                                     Lousy product.      0   amzn\n",
       "974  This phone tries very hard to do everything bu...      0   amzn\n",
       "975  It is the best charger I have seen on the mark...      1   amzn\n",
       "976                                  SWEETEST PHONE!!!      1   amzn\n",
       "977             :-)Oh, the charger seems to work fine.      1   amzn\n",
       "978  It fits so securely that the ear hook does not...      1   amzn\n",
       "979                                 Not enough volume.      0   amzn\n",
       "980                Echo Problem....Very unsatisfactory      0   amzn\n",
       "981  you could only take 2 videos at a time and the...      0   amzn\n",
       "982                            don't waste your money.      0   amzn\n",
       "983  I am going to have to be the first to negative...      0   amzn\n",
       "984  Adapter does not provide enough charging current.      0   amzn\n",
       "985  There was so much hype over this phone that I ...      0   amzn\n",
       "986  You also cannot take pictures with it in the c...      0   amzn\n",
       "987                            Phone falls out easily.      0   amzn\n",
       "988  It didn't work, people can not hear me when I ...      0   amzn\n",
       "989  The text messaging feature is really tricky to...      0   amzn\n",
       "990  I'm really disappointed all I have now is a ch...      0   amzn\n",
       "991                                Painful on the ear.      0   amzn\n",
       "992                   Lasted one day and then blew up.      0   amzn\n",
       "993                                      disappointed.      0   amzn\n",
       "994                              Kind of flops around.      0   amzn\n",
       "995  The screen does get smudged easily because it ...      0   amzn\n",
       "996  What a piece of junk.. I lose more calls on th...      0   amzn\n",
       "997                       Item Does Not Match Picture.      0   amzn\n",
       "998  The only thing that disappoint me is the infra...      0   amzn\n",
       "999  You can not answer calls with the unit, never ...      0   amzn\n",
       "\n",
       "[2748 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "We will perform the following steps:\n",
    "\n",
    "> **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "\n",
    "> Words that have fewer than 3 characters are removed.\n",
    "\n",
    "> All **stopwords** are removed.\n",
    "\n",
    "> Words are **lemmatized** — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "\n",
    "> Words are **stemmed** — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading gensim and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/info/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to perform lemmatize and stem preprocessing steps on the data set.\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I great reception all the time.\n",
      "['great', 'recept', 'time']\n"
     ]
    }
   ],
   "source": [
    "### Test on sample\n",
    "\n",
    "doc_sample = df['sentence'].sample().values[0]\n",
    "print(doc_sample)\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = df['sentence'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        [love, place]\n",
       "1                                        [crust, good]\n",
       "2                               [tasti, textur, nasti]\n",
       "3    [stop, late, bank, holiday, rick, steve, recom...\n",
       "4                         [select, menu, great, price]\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 glad\n",
      "124 waiter\n",
      "1182 neglig\n",
      "2870 raver\n",
      "375 strip\n",
      "473 overcook\n",
      "1318 carb\n",
      "1388 budget\n",
      "129 moist\n",
      "440 avoid\n",
      "2647 mark\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim filter_extremes\n",
    "\n",
    "Filter out tokens that appear in\n",
    "\n",
    "> less than 15 documents (absolute number) or\n",
    "\n",
    "> more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "\n",
    "> after the above two steps, keep only the first 100000 most frequent tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 3 (\"recommend\") appears 1 time.\n",
      "Word 13 (\"time\") appears 1 time.\n",
      "Word 71 (\"wast\") appears 1 time.\n",
      "Word 157 (\"film\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "## Preview Bag Of Words for our sample preprocessed document\n",
    "\n",
    "bow_doc_1234 = bow_corpus[1234]\n",
    "for i in range(len(bow_doc_1234)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1234[i][0], \n",
    "                                               dictionary[bow_doc_1234[i][0]], \n",
    "bow_doc_1234[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7436404725978057), (1, 0.6685797241275809)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.091*\"film\" + 0.062*\"phone\" + 0.052*\"best\" + 0.040*\"love\" + 0.035*\"time\" + 0.025*\"amaz\" + 0.021*\"wast\" + 0.021*\"movi\" + 0.021*\"hear\" + 0.021*\"qualiti\"\n",
      "Topic: 1 \n",
      "Words: 0.068*\"disappoint\" + 0.066*\"movi\" + 0.059*\"pretti\" + 0.037*\"film\" + 0.036*\"great\" + 0.034*\"servic\" + 0.026*\"time\" + 0.024*\"best\" + 0.022*\"total\" + 0.021*\"real\"\n",
      "Topic: 2 \n",
      "Words: 0.118*\"good\" + 0.099*\"place\" + 0.045*\"food\" + 0.030*\"time\" + 0.030*\"great\" + 0.029*\"better\" + 0.025*\"work\" + 0.024*\"get\" + 0.022*\"charger\" + 0.022*\"money\"\n",
      "Topic: 3 \n",
      "Words: 0.053*\"qualiti\" + 0.053*\"think\" + 0.051*\"sound\" + 0.050*\"know\" + 0.043*\"come\" + 0.042*\"wonder\" + 0.035*\"piec\" + 0.024*\"hold\" + 0.021*\"charg\" + 0.021*\"work\"\n",
      "Topic: 4 \n",
      "Words: 0.057*\"price\" + 0.056*\"good\" + 0.047*\"movi\" + 0.034*\"time\" + 0.033*\"long\" + 0.032*\"place\" + 0.031*\"recommend\" + 0.028*\"film\" + 0.022*\"suck\" + 0.022*\"right\"\n",
      "Topic: 5 \n",
      "Words: 0.075*\"go\" + 0.063*\"food\" + 0.046*\"good\" + 0.038*\"movi\" + 0.036*\"excel\" + 0.035*\"want\" + 0.033*\"order\" + 0.032*\"watch\" + 0.027*\"time\" + 0.025*\"delici\"\n",
      "Topic: 6 \n",
      "Words: 0.045*\"good\" + 0.042*\"act\" + 0.041*\"thing\" + 0.038*\"problem\" + 0.030*\"movi\" + 0.029*\"recommend\" + 0.025*\"film\" + 0.023*\"littl\" + 0.023*\"think\" + 0.022*\"love\"\n",
      "Topic: 7 \n",
      "Words: 0.104*\"like\" + 0.051*\"film\" + 0.037*\"movi\" + 0.036*\"charact\" + 0.026*\"look\" + 0.020*\"nice\" + 0.020*\"perform\" + 0.019*\"feel\" + 0.018*\"time\" + 0.018*\"play\"\n",
      "Topic: 8 \n",
      "Words: 0.079*\"work\" + 0.076*\"movi\" + 0.073*\"product\" + 0.039*\"friendli\" + 0.035*\"great\" + 0.029*\"come\" + 0.025*\"tri\" + 0.022*\"like\" + 0.019*\"help\" + 0.018*\"excel\"\n",
      "Topic: 9 \n",
      "Words: 0.149*\"great\" + 0.113*\"phone\" + 0.069*\"servic\" + 0.033*\"work\" + 0.029*\"batteri\" + 0.020*\"sound\" + 0.017*\"qualiti\" + 0.017*\"give\" + 0.017*\"right\" + 0.017*\"worst\"\n"
     ]
    }
   ],
   "source": [
    "## For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.055*\"place\" + 0.051*\"recommend\" + 0.051*\"disappoint\" + 0.048*\"film\" + 0.041*\"delici\" + 0.031*\"thing\" + 0.028*\"easi\" + 0.026*\"servic\" + 0.025*\"dish\" + 0.023*\"phone\"\n",
      "Topic: 1 Word: 0.056*\"time\" + 0.051*\"product\" + 0.038*\"good\" + 0.029*\"piec\" + 0.029*\"charact\" + 0.027*\"item\" + 0.027*\"break\" + 0.021*\"film\" + 0.020*\"receiv\" + 0.020*\"pictur\"\n",
      "Topic: 2 Word: 0.051*\"movi\" + 0.044*\"pretti\" + 0.039*\"better\" + 0.031*\"wait\" + 0.028*\"worth\" + 0.027*\"cool\" + 0.027*\"film\" + 0.025*\"good\" + 0.024*\"end\" + 0.023*\"interest\"\n",
      "Topic: 3 Word: 0.119*\"phone\" + 0.033*\"case\" + 0.031*\"tri\" + 0.031*\"go\" + 0.030*\"highli\" + 0.030*\"great\" + 0.030*\"want\" + 0.027*\"work\" + 0.025*\"enjoy\" + 0.024*\"perform\"\n",
      "Topic: 4 Word: 0.064*\"place\" + 0.060*\"food\" + 0.046*\"money\" + 0.042*\"order\" + 0.041*\"purchas\" + 0.032*\"problem\" + 0.027*\"wast\" + 0.026*\"total\" + 0.025*\"avoid\" + 0.023*\"funni\"\n",
      "Topic: 5 Word: 0.109*\"good\" + 0.046*\"best\" + 0.041*\"great\" + 0.037*\"star\" + 0.034*\"work\" + 0.034*\"come\" + 0.033*\"year\" + 0.029*\"right\" + 0.023*\"servic\" + 0.022*\"qualiti\"\n",
      "Topic: 6 Word: 0.077*\"love\" + 0.040*\"servic\" + 0.037*\"worst\" + 0.034*\"look\" + 0.029*\"terribl\" + 0.029*\"think\" + 0.027*\"time\" + 0.027*\"good\" + 0.025*\"return\" + 0.022*\"sound\"\n",
      "Topic: 7 Word: 0.115*\"movi\" + 0.056*\"disappoint\" + 0.056*\"work\" + 0.039*\"act\" + 0.030*\"aw\" + 0.027*\"definit\" + 0.026*\"watch\" + 0.024*\"wast\" + 0.023*\"understand\" + 0.022*\"awesom\"\n",
      "Topic: 8 Word: 0.066*\"like\" + 0.060*\"film\" + 0.043*\"nice\" + 0.034*\"get\" + 0.034*\"phone\" + 0.031*\"amaz\" + 0.029*\"batteri\" + 0.024*\"flavor\" + 0.024*\"price\" + 0.024*\"work\"\n",
      "Topic: 9 Word: 0.132*\"great\" + 0.038*\"real\" + 0.033*\"say\" + 0.032*\"friendli\" + 0.028*\"good\" + 0.027*\"scene\" + 0.026*\"food\" + 0.025*\"beauti\" + 0.023*\"like\" + 0.022*\"staff\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation by classifying using LDA bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4707123637199402\t \n",
      "Topic: 0.091*\"film\" + 0.062*\"phone\" + 0.052*\"best\" + 0.040*\"love\" + 0.035*\"time\" + 0.025*\"amaz\" + 0.021*\"wast\" + 0.021*\"movi\" + 0.021*\"hear\" + 0.021*\"qualiti\"\n",
      "\n",
      "Score: 0.3692642152309418\t \n",
      "Topic: 0.057*\"price\" + 0.056*\"good\" + 0.047*\"movi\" + 0.034*\"time\" + 0.033*\"long\" + 0.032*\"place\" + 0.031*\"recommend\" + 0.028*\"film\" + 0.022*\"suck\" + 0.022*\"right\"\n",
      "\n",
      "Score: 0.02000550925731659\t \n",
      "Topic: 0.045*\"good\" + 0.042*\"act\" + 0.041*\"thing\" + 0.038*\"problem\" + 0.030*\"movi\" + 0.029*\"recommend\" + 0.025*\"film\" + 0.023*\"littl\" + 0.023*\"think\" + 0.022*\"love\"\n",
      "\n",
      "Score: 0.020003627985715866\t \n",
      "Topic: 0.104*\"like\" + 0.051*\"film\" + 0.037*\"movi\" + 0.036*\"charact\" + 0.026*\"look\" + 0.020*\"nice\" + 0.020*\"perform\" + 0.019*\"feel\" + 0.018*\"time\" + 0.018*\"play\"\n",
      "\n",
      "Score: 0.02000354416668415\t \n",
      "Topic: 0.075*\"go\" + 0.063*\"food\" + 0.046*\"good\" + 0.038*\"movi\" + 0.036*\"excel\" + 0.035*\"want\" + 0.033*\"order\" + 0.032*\"watch\" + 0.027*\"time\" + 0.025*\"delici\"\n",
      "\n",
      "Score: 0.020003531128168106\t \n",
      "Topic: 0.118*\"good\" + 0.099*\"place\" + 0.045*\"food\" + 0.030*\"time\" + 0.030*\"great\" + 0.029*\"better\" + 0.025*\"work\" + 0.024*\"get\" + 0.022*\"charger\" + 0.022*\"money\"\n",
      "\n",
      "Score: 0.020003123208880424\t \n",
      "Topic: 0.068*\"disappoint\" + 0.066*\"movi\" + 0.059*\"pretti\" + 0.037*\"film\" + 0.036*\"great\" + 0.034*\"servic\" + 0.026*\"time\" + 0.024*\"best\" + 0.022*\"total\" + 0.021*\"real\"\n",
      "\n",
      "Score: 0.020002348348498344\t \n",
      "Topic: 0.079*\"work\" + 0.076*\"movi\" + 0.073*\"product\" + 0.039*\"friendli\" + 0.035*\"great\" + 0.029*\"come\" + 0.025*\"tri\" + 0.022*\"like\" + 0.019*\"help\" + 0.018*\"excel\"\n",
      "\n",
      "Score: 0.020001020282506943\t \n",
      "Topic: 0.149*\"great\" + 0.113*\"phone\" + 0.069*\"servic\" + 0.033*\"work\" + 0.029*\"batteri\" + 0.020*\"sound\" + 0.017*\"qualiti\" + 0.017*\"give\" + 0.017*\"right\" + 0.017*\"worst\"\n",
      "\n",
      "Score: 0.020000720396637917\t \n",
      "Topic: 0.053*\"qualiti\" + 0.053*\"think\" + 0.051*\"sound\" + 0.050*\"know\" + 0.043*\"come\" + 0.042*\"wonder\" + 0.035*\"piec\" + 0.024*\"hold\" + 0.021*\"charg\" + 0.021*\"work\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1234]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation by classifying using LDA TD-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4767487645149231\t \n",
      "Topic: 0.055*\"place\" + 0.051*\"recommend\" + 0.051*\"disappoint\" + 0.048*\"film\" + 0.041*\"delici\" + 0.031*\"thing\" + 0.028*\"easi\" + 0.026*\"servic\" + 0.025*\"dish\" + 0.023*\"phone\"\n",
      "\n",
      "Score: 0.3632095158100128\t \n",
      "Topic: 0.064*\"place\" + 0.060*\"food\" + 0.046*\"money\" + 0.042*\"order\" + 0.041*\"purchas\" + 0.032*\"problem\" + 0.027*\"wast\" + 0.026*\"total\" + 0.025*\"avoid\" + 0.023*\"funni\"\n",
      "\n",
      "Score: 0.02001214399933815\t \n",
      "Topic: 0.056*\"time\" + 0.051*\"product\" + 0.038*\"good\" + 0.029*\"piec\" + 0.029*\"charact\" + 0.027*\"item\" + 0.027*\"break\" + 0.021*\"film\" + 0.020*\"receiv\" + 0.020*\"pictur\"\n",
      "\n",
      "Score: 0.020008111372590065\t \n",
      "Topic: 0.077*\"love\" + 0.040*\"servic\" + 0.037*\"worst\" + 0.034*\"look\" + 0.029*\"terribl\" + 0.029*\"think\" + 0.027*\"time\" + 0.027*\"good\" + 0.025*\"return\" + 0.022*\"sound\"\n",
      "\n",
      "Score: 0.020006146281957626\t \n",
      "Topic: 0.066*\"like\" + 0.060*\"film\" + 0.043*\"nice\" + 0.034*\"get\" + 0.034*\"phone\" + 0.031*\"amaz\" + 0.029*\"batteri\" + 0.024*\"flavor\" + 0.024*\"price\" + 0.024*\"work\"\n",
      "\n",
      "Score: 0.020005134865641594\t \n",
      "Topic: 0.115*\"movi\" + 0.056*\"disappoint\" + 0.056*\"work\" + 0.039*\"act\" + 0.030*\"aw\" + 0.027*\"definit\" + 0.026*\"watch\" + 0.024*\"wast\" + 0.023*\"understand\" + 0.022*\"awesom\"\n",
      "\n",
      "Score: 0.020005013793706894\t \n",
      "Topic: 0.051*\"movi\" + 0.044*\"pretti\" + 0.039*\"better\" + 0.031*\"wait\" + 0.028*\"worth\" + 0.027*\"cool\" + 0.027*\"film\" + 0.025*\"good\" + 0.024*\"end\" + 0.023*\"interest\"\n",
      "\n",
      "Score: 0.020002776756882668\t \n",
      "Topic: 0.119*\"phone\" + 0.033*\"case\" + 0.031*\"tri\" + 0.031*\"go\" + 0.030*\"highli\" + 0.030*\"great\" + 0.030*\"want\" + 0.027*\"work\" + 0.025*\"enjoy\" + 0.024*\"perform\"\n",
      "\n",
      "Score: 0.020001810044050217\t \n",
      "Topic: 0.109*\"good\" + 0.046*\"best\" + 0.041*\"great\" + 0.037*\"star\" + 0.034*\"work\" + 0.034*\"come\" + 0.033*\"year\" + 0.029*\"right\" + 0.023*\"servic\" + 0.022*\"qualiti\"\n",
      "\n",
      "Score: 0.020000603049993515\t \n",
      "Topic: 0.132*\"great\" + 0.038*\"real\" + 0.033*\"say\" + 0.032*\"friendli\" + 0.028*\"good\" + 0.027*\"scene\" + 0.026*\"food\" + 0.025*\"beauti\" + 0.023*\"like\" + 0.022*\"staff\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[1234]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
